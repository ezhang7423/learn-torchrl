{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ezipe/git/learn-torchrl/.venv/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_envs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_envs` for environment variables or `env.get_wrapper_attr('num_envs')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/ezipe/git/learn-torchrl/.venv/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward_space` for environment variables or `env.get_wrapper_attr('reward_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "from torchrl.envs import GymEnv\n",
    "\n",
    "env = GymEnv(\"Pendulum-v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.modules import ProbabilisticActor, MLP, NormalParamWrapper, TanhNormal\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.modules.tensordict_module.actors import ValueOperator\n",
    "from torch import nn\n",
    "\n",
    "from learning_torchrl import Config\n",
    "\n",
    "conf = Config()\n",
    "\n",
    "# TODO not sure if this is quite correct\n",
    "n_obs = env.observation_space.shape[0]\n",
    "policy = ProbabilisticActor(\n",
    "    TensorDictModule(\n",
    "        NormalParamWrapper(\n",
    "            MLP(\n",
    "                in_features=n_obs,\n",
    "                out_features=env.action_space.shape[0] * 2, # mean and std\n",
    "                num_cells=[conf.policy_hidden] * conf.policy_layers,\n",
    "            ),\n",
    "        ),\n",
    "        in_keys=[\"obs\"],\n",
    "        out_keys=[\"loc\", \"scale\"],\n",
    "    ),\n",
    "    in_keys=[\"loc\", \"scale\"],\n",
    "    distribution_class=TanhNormal,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        obs: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "    batch_size=torch.Size([10, 3]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.from_dict({\"obs\": torch.randn(10, n_obs)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0134],\n",
       "        [ 0.0108],\n",
       "        [-0.0117],\n",
       "        [ 0.0272],\n",
       "        [ 0.0065],\n",
       "        [ 0.0346],\n",
       "        [ 0.0302],\n",
       "        [-0.0175],\n",
       "        [ 0.0219],\n",
       "        [-0.0223]], grad_fn=<SplitBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from tensordict import TensorDict as td\n",
    "\n",
    "nn = MLP(\n",
    "    in_features=n_obs,\n",
    "    out_features=env.action_space.shape[0] * 2,\n",
    "    num_cells=[conf.policy_hidden] * conf.policy_layers,\n",
    ")\n",
    "\n",
    "# nn(torch.randn(10, n_obs))\n",
    "\n",
    "td_nn = TensorDictModule(NormalParamWrapper(nn), in_keys=['obs'], out_keys=['loc', 'scale'])\n",
    "\n",
    "# td_nn(td({'obs': torch.randn(10, n_obs)}, batch_size=10)) # now it can take in a tensordict\n",
    "# td_nn(torch.randn(10, n_obs))\n",
    "\n",
    "policy = ProbabilisticActor(td_nn, in_keys=['loc', 'scale'], distribution_class=torch.distributions.Normal) # now it samples from a distribution paramereterized by the output of the nn\n",
    "policy(td({\"obs\": torch.randn(10, n_obs)}, batch_size=10))['action']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
